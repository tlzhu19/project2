{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 67364 Final Project: Predicting Crime in US Cities\n",
    "Tiffany Zhu and Justin Chan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Question\n",
    "\n",
    "We are interested in seeing if we can predict crime rates of cities in the US. We want to know what factors contribute to the crime rate, for example household income and demographic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Data Set\n",
    "\n",
    "Our data set can be found on the UCI Machine Learning Repository (http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalize). The data contains information from 2215 different cities in the US. Each city has 147 attributes population demographics, income information, housing information and police information such as crime rates and the demographics of the police force. The demographic, income and housing information was from the 1990 Census and police information was taken from a 1995 Law Enforcement Survey. Additionally, as we will see below, several of the attributes contain NAs for most cities, so we have to initally remove several attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we grab the data set and don't include the features that contain NAs in most of the instances (cities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as pp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "feature_matrix = []\n",
    "target_vector1 = []\n",
    "target_vector2 = []\n",
    "varToNumNA = dict()\n",
    "\n",
    "for line in open('CommViolPredUnnormalizedData.txt', 'r'):\n",
    "    features_orig = line.strip().split(',')\n",
    "    for i in range(len(features_orig)):\n",
    "        if features_orig[i] == '?':\n",
    "            try:\n",
    "                varToNumNA[i] += 1\n",
    "            except:\n",
    "                varToNumNA[i] = 1\n",
    "    \n",
    "    target1 = features_orig[-2] # ViolentCrimesPerPop\n",
    "    target2 = features_orig[-1] # nonViolPerPop\n",
    "    #features = [ f for f in features[3:-2]] # don't include town and state name\n",
    "    features = [ f for f in features_orig[4:103] ] #don't include 103\n",
    "    features += [f for f in features_orig[120:123]] #don't include 103-119, or 123\n",
    "    features += [features_orig[127]] \n",
    "    features += [f for f in features_orig[129:131]]\n",
    "    features += [f for f in features_orig[133:143]]\n",
    "    feature_matrix.append(features)\n",
    "    target_vector1.append(target1)\n",
    "    target_vector2.append(target2)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function statistical_measures below is taken from class. We will use this later to exam our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "import numpy as np\n",
    "\n",
    "# http://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n",
    "def statistical_measures(confusion_matrix):\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)  \n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "\n",
    "    # Overall accuracy\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return {'TPR':TPR, 'TNR':TNR, 'PPV':PPV, 'NPV':NPV, 'FPR':FPR, 'FNR':FNR, 'FDR':FDR, 'ACC':ACC}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see all of the variables that contained NAs. So we get a sense that there are several variables that we can initially remove so that when we actually go to fit a model, we won't have to remove rows from the data (due to NAs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 1221,\n",
       " 3: 1224,\n",
       " 30: 1,\n",
       " 103: 1872,\n",
       " 104: 1872,\n",
       " 105: 1872,\n",
       " 106: 1872,\n",
       " 107: 1872,\n",
       " 108: 1872,\n",
       " 109: 1872,\n",
       " 110: 1872,\n",
       " 111: 1872,\n",
       " 112: 1872,\n",
       " 113: 1872,\n",
       " 114: 1872,\n",
       " 115: 1872,\n",
       " 116: 1872,\n",
       " 117: 1872,\n",
       " 118: 1872,\n",
       " 119: 1872,\n",
       " 123: 1872,\n",
       " 124: 1872,\n",
       " 125: 1872,\n",
       " 126: 1872,\n",
       " 128: 1872,\n",
       " 131: 208,\n",
       " 132: 208,\n",
       " 133: 1,\n",
       " 134: 1,\n",
       " 135: 13,\n",
       " 136: 13,\n",
       " 137: 3,\n",
       " 138: 3,\n",
       " 139: 3,\n",
       " 140: 3,\n",
       " 141: 3,\n",
       " 142: 3,\n",
       " 143: 91,\n",
       " 144: 91,\n",
       " 145: 221,\n",
       " 146: 97}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't use the variables that have a lot of '?'s in th data\n",
    "varToNumNA # {var : numNA}, var is the index of the variable, numNA is the nubmer of ?s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix[1]\n",
    "'?' in feature_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can fit a model to our data set, we must remove the NAs ('?' in the data set) in order to use functions in SKlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_matrix_clean = []\n",
    "target_vector1_clean = []\n",
    "target_vector2_clean = []\n",
    "for i in range(len(feature_matrix)):\n",
    "    if ('?' not in feature_matrix[i] and '?' not in target_vector1[i] and '?' not in target_vector2[i]):\n",
    "        feature_matrix_clean.append([float(x) for x in feature_matrix[i]])\n",
    "        target_vector1_clean.append(float(target_vector1[i]))\n",
    "        target_vector2_clean.append(float(target_vector2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2215, 1901)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_matrix), len(feature_matrix_clean) # get rid of some data ~300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia, we find the the average crime rate in the US is 636.6. 636.6 is total number of violent crimes per 100K people. So we set a variable for this. Our goal is to predict if a city has a crime rate higher than the national average. We will create a binary variable (our target variable that we want to predict) that will be 1 if the crime rate is above average and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AVG_CRIME = 636.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   1.19800000e+04   3.10000000e+00 ...,   1.13208000e+03\n",
      "    1.60000000e+01   1.31260000e+02]\n",
      " [  1.00000000e+00   2.31230000e+04   2.82000000e+00 ...,   1.59878000e+03\n",
      "    2.60000000e+01   1.10550000e+02]\n",
      " [  1.00000000e+00   2.93440000e+04   2.43000000e+00 ...,   4.97219000e+03\n",
      "    1.36000000e+02   3.76300000e+02]\n",
      " ..., \n",
      " [  1.00000000e+01   3.28240000e+04   2.46000000e+00 ...,   2.43597000e+03\n",
      "    1.79000000e+02   4.87190000e+02]\n",
      " [  1.00000000e+01   1.35470000e+04   2.89000000e+00 ...,   3.72290000e+03\n",
      "    1.30000000e+01   1.02100000e+02]\n",
      " [  1.00000000e+01   2.88980000e+04   2.61000000e+00 ...,   4.81920000e+03\n",
      "    4.05000000e+02   1.33867000e+03]]\n",
      "[0 0 0 ..., 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "data = np.array( feature_matrix_clean )\n",
    "target1 = np.array( [ (1 if (x > AVG_CRIME) else 0) for x in target_vector1_clean] )\n",
    "target2 = np.array( [ (1 if (x > AVG_CRIME) else 0) for x in target_vector2_clean] )\n",
    "\n",
    "print(data)\n",
    "print(target1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Naive Bayes\n",
    "\n",
    "We first try fitting a Naive Bayes model using sklearn.naive_bayes. We get an accuracy or 0.867, or an error rate of 0.133, which seems quite good. We also note that our base rate is around 0.31 (if we guessed 0 for every city, we would get 31% wrong). So already, 0.133 compared to 0.31 seems quite good. We also look at the confusion matrix further down. We want the false poitive and false negative rates to be low and the precision, true positive and true negative rates to be high. This seems like the case in our confusion matrix. But the false negative rate seems a bit high (i.e. the true positive rate seems a bit low). So, the times we predicted that the city does not have above average crime rates but they actually do, is a bit high. This may be concerning because if we told a family who was looking for a nice area that Camden, NJ has below average crime rate, but actually it has above average, this would be a problem. This family would unknownly think Camden is an OK place to be, in terms of crime rates. Thus we will explore other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31351920042083115"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rate = sum(target1) / len(target1)\n",
    "base_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use a variation of NB \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# fit a Naive Bayes model to the data\n",
    "model = GaussianNB()\n",
    "X_train, y_train1 = data, target1 \n",
    "model.fit(X_train, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_train) \n",
    "y_expected = y_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.867964229353\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.97      0.91      1305\n",
      "          1       0.90      0.65      0.76       596\n",
      "\n",
      "avg / total       0.87      0.87      0.86      1901\n",
      "\n",
      "[[1263   42]\n",
      " [ 209  387]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import  metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# summarize the fit of the model\n",
    "\n",
    "print(metrics.accuracy_score(y_expected, y_predicted))\n",
    "print()\n",
    "print(metrics.classification_report(y_expected, y_predicted))\n",
    "print(metrics.confusion_matrix(y_expected, y_predicted))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tiffanyzhu/miniconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.864808,  0.864808]),\n",
       " 'FDR': array([ 0.14109589,  0.11564626]),\n",
       " 'FNR': array([ 0.03908046,  0.34563758]),\n",
       " 'FPR': array([ 0.34563758,  0.03908046]),\n",
       " 'NPV': array([ 0.88435374,  0.85890411]),\n",
       " 'PPV': array([ 0.85890411,  0.88435374]),\n",
       " 'TNR': array([ 0.65436242,  0.96091954]),\n",
       " 'TPR': array([ 0.96091954,  0.65436242])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "predict0 = cross_validation.cross_val_predict(GaussianNB(), X_train, y_train1, cv=10)\n",
    "cm0 = confusion_matrix(y_expected, predict0)\n",
    "statistical_measures(cm0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readFile(filename, mode=\"rt\"):\n",
    "    # rt stands for \"read text\"\n",
    "    fin = contents = None\n",
    "    try:\n",
    "        fin = open(filename, mode)\n",
    "        contents = fin.read()\n",
    "    finally:\n",
    "        if (fin != None): fin.close()\n",
    "    return contents\n",
    "\n",
    "#def indexToName(i):\n",
    "#    contents = readFile('varNames.txt')\n",
    "#    contents_list = contents.split('\\n')\n",
    "#    contents_list = [ (s.split())[1][:-1] for s in contents_list ]\n",
    "#    return contents_list[i]\n",
    "\n",
    "# get all of the variable names\n",
    "contents = readFile('varNames.txt')\n",
    "contents_list = contents.split('\\n')\n",
    "contents_list = [ (s.split())[1][:-1] for s in contents_list ]\n",
    "#contents_list.index('population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at another model, we want to just quickly visualize which variables are the ones we removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1221, 'countyCode'),\n",
       " (3, 1224, 'communityCode'),\n",
       " (30, 1, 'OtherPerCap'),\n",
       " (103, 1872, 'LemasSwornFT'),\n",
       " (104, 1872, 'LemasSwFTPerPop'),\n",
       " (105, 1872, 'LemasSwFTFieldOps'),\n",
       " (106, 1872, 'LemasSwFTFieldPerPop'),\n",
       " (107, 1872, 'LemasTotalReq'),\n",
       " (108, 1872, 'LemasTotReqPerPop'),\n",
       " (109, 1872, 'PolicReqPerOffic'),\n",
       " (110, 1872, 'PolicPerPop'),\n",
       " (111, 1872, 'RacialMatchCommPol'),\n",
       " (112, 1872, 'PctPolicWhite'),\n",
       " (113, 1872, 'PctPolicBlack'),\n",
       " (114, 1872, 'PctPolicHisp'),\n",
       " (115, 1872, 'PctPolicAsian'),\n",
       " (116, 1872, 'PctPolicMinor'),\n",
       " (117, 1872, 'OfficAssgnDrugUnits'),\n",
       " (118, 1872, 'NumKindsDrugsSeiz'),\n",
       " (119, 1872, 'PolicAveOTWorked'),\n",
       " (123, 1872, 'PolicCars'),\n",
       " (124, 1872, 'PolicOperBudg'),\n",
       " (125, 1872, 'LemasPctPolicOnPatr'),\n",
       " (126, 1872, 'LemasGangUnitDeploy'),\n",
       " (128, 1872, 'PolicBudgPerPop'),\n",
       " (131, 208, 'rapes'),\n",
       " (132, 208, 'rapesPerPop'),\n",
       " (133, 1, 'robberies'),\n",
       " (134, 1, 'robbbPerPop'),\n",
       " (135, 13, 'assaults'),\n",
       " (136, 13, 'assaultPerPop'),\n",
       " (137, 3, 'burglaries'),\n",
       " (138, 3, 'burglPerPop'),\n",
       " (139, 3, 'larcenies'),\n",
       " (140, 3, 'larcPerPop'),\n",
       " (141, 3, 'autoTheft'),\n",
       " (142, 3, 'autoTheftPerPop'),\n",
       " (143, 91, 'arsons'),\n",
       " (144, 91, 'arsonsPerPop'),\n",
       " (145, 221, 'ViolentCrimesPerPop'),\n",
       " (146, 97, 'nonViolPerPop')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varNames = []\n",
    "for i in varToNumNA:\n",
    "    varNames += [(i, varToNumNA[i], contents_list[i])]\n",
    "sorted(varNames) # variables that we didn't use: (index, # of times used, var name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logistic Regression (L1 and L2 penalty)\n",
    "\n",
    "We next try logistic regression using L1 and L2 penalty. We also will use cross-validated results (using 10 folds) because this will give us more accurate results and we can see how well we are doing. We do very similar to what we did in section 3.1. After fitting a logistic regression model, we get the predictions and find the confusion matrix. Comparing the model using L1 penalty with the model using L2 penalty, we actually get quite similar results. However, comparing logistic regression with Naive Bayes, logistic seems to be better. Looking at the cross-validated results, our error rate for logistic with L1 penalty is about 0.025, with L2 penaltiy it is about 0.022. This is better than Naive Bayes (0.133) and definitely greatly better than the base rate (0.31). The false negative rate is also a lot lower, 0.047 and 0.037 (L1 and L2) compared to 0.345. So using logistic regression seems like a better model, however we will explore another model, random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "# use logistic reg and L1 penalty \n",
    "logreg = linear_model.LogisticRegression(C=1e5, penalty='l1',)\n",
    "X = feature_matrix_clean\n",
    "y = [ (1 if (x > AVG_CRIME) else 0) for x in target_vector1_clean]\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993687532877\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00      1305\n",
      "          1       1.00      0.98      0.99       596\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1901\n",
      "\n",
      "[[1303    2]\n",
      " [  10  586]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predicted_log = logreg.predict(X)\n",
    "y_expected = y_train1\n",
    "print(metrics.accuracy_score(y_expected, y_predicted_log))\n",
    "print()\n",
    "print(metrics.classification_report(y_expected, y_predicted_log))\n",
    "print(metrics.confusion_matrix(y_expected, y_predicted_log))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97475013151\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98      1305\n",
      "          1       0.97      0.95      0.96       596\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logistic regression w/ L1 penalty and CV\n",
    "predicted = cross_validation.cross_val_predict(linear_model.LogisticRegression(penalty='l1'), X, y, cv=10)\n",
    "print(metrics.accuracy_score(y, predicted))\n",
    "print(metrics.classification_report(y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.97475013,  0.97475013]),\n",
       " 'FDR': array([ 0.02132521,  0.03401361]),\n",
       " 'FNR': array([ 0.01532567,  0.04697987]),\n",
       " 'FPR': array([ 0.04697987,  0.01532567]),\n",
       " 'NPV': array([ 0.96598639,  0.97867479]),\n",
       " 'PPV': array([ 0.97867479,  0.96598639]),\n",
       " 'TNR': array([ 0.95302013,  0.98467433]),\n",
       " 'TPR': array([ 0.98467433,  0.95302013])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l1 log reg, CV\n",
    "cm2 = confusion_matrix(y, predicted)\n",
    "statistical_measures(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.990005260389\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      1305\n",
      "          1       0.99      0.98      0.98       596\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1901\n",
      "\n",
      "[[1299    6]\n",
      " [  13  583]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use logistic reg and L2 penalty \n",
    "logreg2 = linear_model.LogisticRegression(C=1e5, penalty='l2',)\n",
    "logreg2.fit(X, y)\n",
    "\n",
    "y_predicted_log2 = logreg2.predict(X)\n",
    "print(metrics.accuracy_score(y_expected, y_predicted_log2))\n",
    "print()\n",
    "print(metrics.classification_report(y_expected, y_predicted_log2))\n",
    "print(metrics.confusion_matrix(y_expected, y_predicted_log2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.978432403998\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.98      1305\n",
      "          1       0.97      0.96      0.97       596\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted2 = cross_validation.cross_val_predict(linear_model.LogisticRegression(penalty='l2'), X, y, cv=10)\n",
    "print(metrics.accuracy_score(y, predicted2))\n",
    "print(metrics.classification_report(y, predicted2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.9784324,  0.9784324]),\n",
       " 'FDR': array([ 0.01681957,  0.03204047]),\n",
       " 'FNR': array([ 0.01455939,  0.03691275]),\n",
       " 'FPR': array([ 0.03691275,  0.01455939]),\n",
       " 'NPV': array([ 0.96795953,  0.98318043]),\n",
       " 'PPV': array([ 0.98318043,  0.96795953]),\n",
       " 'TNR': array([ 0.96308725,  0.98544061]),\n",
       " 'TPR': array([ 0.98544061,  0.96308725])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l2 log reg, CV\n",
    "cm3 = confusion_matrix(y, predicted2)\n",
    "statistical_measures(cm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest\n",
    "Below, we try a random forest. We fit the model using cross-validation and again find the confusion matrix. Using a random forest, we actually get worse results compared to Naive Bayes and logistic regression. Our error rate is around 0.16 (comapred to 0.14 for Naive Bayes and around 0.03 for logistic regression), but it is still better than the base rate, 0.31. Similar to when we used Naive Bayes, the false negative rate is quite high but actually not as high as Naive Bayes (0.297 vs 0.345). This again would be a problem because of what we said in section 3.1. It could be a big problem to falsely say that a city has low crime rate when in fact it actually has high crime rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BerkeleyHeightstownship</td>\n",
       "      <td>NJ</td>\n",
       "      <td>39</td>\n",
       "      <td>5320</td>\n",
       "      <td>1</td>\n",
       "      <td>11980</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>91.78</td>\n",
       "      <td>6.50</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>114.85</td>\n",
       "      <td>138</td>\n",
       "      <td>1132.08</td>\n",
       "      <td>16</td>\n",
       "      <td>131.26</td>\n",
       "      <td>2</td>\n",
       "      <td>16.41</td>\n",
       "      <td>41.02</td>\n",
       "      <td>1394.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marpletownship</td>\n",
       "      <td>PA</td>\n",
       "      <td>45</td>\n",
       "      <td>47616</td>\n",
       "      <td>1</td>\n",
       "      <td>23123</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>95.57</td>\n",
       "      <td>3.44</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>242.37</td>\n",
       "      <td>376</td>\n",
       "      <td>1598.78</td>\n",
       "      <td>26</td>\n",
       "      <td>110.55</td>\n",
       "      <td>1</td>\n",
       "      <td>4.25</td>\n",
       "      <td>127.56</td>\n",
       "      <td>1955.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tigardcity</td>\n",
       "      <td>OR</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>29344</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>94.33</td>\n",
       "      <td>3.43</td>\n",
       "      <td>...</td>\n",
       "      <td>274</td>\n",
       "      <td>758.14</td>\n",
       "      <td>1797</td>\n",
       "      <td>4972.19</td>\n",
       "      <td>136</td>\n",
       "      <td>376.3</td>\n",
       "      <td>22</td>\n",
       "      <td>60.87</td>\n",
       "      <td>218.59</td>\n",
       "      <td>6167.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gloversvillecity</td>\n",
       "      <td>NY</td>\n",
       "      <td>35</td>\n",
       "      <td>29443</td>\n",
       "      <td>1</td>\n",
       "      <td>16656</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.70</td>\n",
       "      <td>97.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>225</td>\n",
       "      <td>1301.78</td>\n",
       "      <td>716</td>\n",
       "      <td>4142.56</td>\n",
       "      <td>47</td>\n",
       "      <td>271.93</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>306.64</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bemidjicity</td>\n",
       "      <td>MN</td>\n",
       "      <td>7</td>\n",
       "      <td>5068</td>\n",
       "      <td>1</td>\n",
       "      <td>11245</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.53</td>\n",
       "      <td>89.16</td>\n",
       "      <td>1.17</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>728.93</td>\n",
       "      <td>1060</td>\n",
       "      <td>8490.87</td>\n",
       "      <td>91</td>\n",
       "      <td>728.93</td>\n",
       "      <td>5</td>\n",
       "      <td>40.05</td>\n",
       "      <td>?</td>\n",
       "      <td>9988.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0   1   2      3    4      5     6     7      8    \\\n",
       "0  BerkeleyHeightstownship  NJ  39   5320    1  11980  3.10  1.37  91.78   \n",
       "1           Marpletownship  PA  45  47616    1  23123  2.82  0.80  95.57   \n",
       "2               Tigardcity  OR   ?      ?    1  29344  2.43  0.74  94.33   \n",
       "3         Gloversvillecity  NY  35  29443    1  16656  2.40  1.70  97.35   \n",
       "4              Bemidjicity  MN   7   5068    1  11245  2.76  0.53  89.16   \n",
       "\n",
       "    9     ...     137      138   139      140  141     142  143    144  \\\n",
       "0  6.50   ...      14   114.85   138  1132.08   16  131.26    2  16.41   \n",
       "1  3.44   ...      57   242.37   376  1598.78   26  110.55    1   4.25   \n",
       "2  3.43   ...     274   758.14  1797  4972.19  136   376.3   22  60.87   \n",
       "3  0.50   ...     225  1301.78   716  4142.56   47  271.93    ?      ?   \n",
       "4  1.17   ...      91   728.93  1060  8490.87   91  728.93    5  40.05   \n",
       "\n",
       "      145      146  \n",
       "0   41.02  1394.59  \n",
       "1  127.56  1955.95  \n",
       "2  218.59  6167.51  \n",
       "3  306.64        ?  \n",
       "4       ?  9988.79  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now try what we did in class on 5/2 (random forest and confusion matrix to analyze)\n",
    "df = pd.read_csv('CommViolPredUnnormalizedData.txt', header=None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>communityname</th>\n",
       "      <th>state</th>\n",
       "      <th>countyCode</th>\n",
       "      <th>communityCode</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>...</th>\n",
       "      <th>burglaries</th>\n",
       "      <th>burglPerPop</th>\n",
       "      <th>larcenies</th>\n",
       "      <th>larcPerPop</th>\n",
       "      <th>autoTheft</th>\n",
       "      <th>autoTheftPerPop</th>\n",
       "      <th>arsons</th>\n",
       "      <th>arsonsPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "      <th>nonViolPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BerkeleyHeightstownship</td>\n",
       "      <td>NJ</td>\n",
       "      <td>39</td>\n",
       "      <td>5320</td>\n",
       "      <td>1</td>\n",
       "      <td>11980</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>91.78</td>\n",
       "      <td>6.50</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>114.85</td>\n",
       "      <td>138</td>\n",
       "      <td>1132.08</td>\n",
       "      <td>16</td>\n",
       "      <td>131.26</td>\n",
       "      <td>2</td>\n",
       "      <td>16.41</td>\n",
       "      <td>41.02</td>\n",
       "      <td>1394.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marpletownship</td>\n",
       "      <td>PA</td>\n",
       "      <td>45</td>\n",
       "      <td>47616</td>\n",
       "      <td>1</td>\n",
       "      <td>23123</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>95.57</td>\n",
       "      <td>3.44</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>242.37</td>\n",
       "      <td>376</td>\n",
       "      <td>1598.78</td>\n",
       "      <td>26</td>\n",
       "      <td>110.55</td>\n",
       "      <td>1</td>\n",
       "      <td>4.25</td>\n",
       "      <td>127.56</td>\n",
       "      <td>1955.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tigardcity</td>\n",
       "      <td>OR</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>29344</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>94.33</td>\n",
       "      <td>3.43</td>\n",
       "      <td>...</td>\n",
       "      <td>274</td>\n",
       "      <td>758.14</td>\n",
       "      <td>1797</td>\n",
       "      <td>4972.19</td>\n",
       "      <td>136</td>\n",
       "      <td>376.3</td>\n",
       "      <td>22</td>\n",
       "      <td>60.87</td>\n",
       "      <td>218.59</td>\n",
       "      <td>6167.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gloversvillecity</td>\n",
       "      <td>NY</td>\n",
       "      <td>35</td>\n",
       "      <td>29443</td>\n",
       "      <td>1</td>\n",
       "      <td>16656</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.70</td>\n",
       "      <td>97.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>225</td>\n",
       "      <td>1301.78</td>\n",
       "      <td>716</td>\n",
       "      <td>4142.56</td>\n",
       "      <td>47</td>\n",
       "      <td>271.93</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>306.64</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bemidjicity</td>\n",
       "      <td>MN</td>\n",
       "      <td>7</td>\n",
       "      <td>5068</td>\n",
       "      <td>1</td>\n",
       "      <td>11245</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.53</td>\n",
       "      <td>89.16</td>\n",
       "      <td>1.17</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>728.93</td>\n",
       "      <td>1060</td>\n",
       "      <td>8490.87</td>\n",
       "      <td>91</td>\n",
       "      <td>728.93</td>\n",
       "      <td>5</td>\n",
       "      <td>40.05</td>\n",
       "      <td>?</td>\n",
       "      <td>9988.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             communityname state countyCode communityCode  fold  population  \\\n",
       "0  BerkeleyHeightstownship    NJ         39          5320     1       11980   \n",
       "1           Marpletownship    PA         45         47616     1       23123   \n",
       "2               Tigardcity    OR          ?             ?     1       29344   \n",
       "3         Gloversvillecity    NY         35         29443     1       16656   \n",
       "4              Bemidjicity    MN          7          5068     1       11245   \n",
       "\n",
       "   householdsize  racepctblack  racePctWhite  racePctAsian      ...        \\\n",
       "0           3.10          1.37         91.78          6.50      ...         \n",
       "1           2.82          0.80         95.57          3.44      ...         \n",
       "2           2.43          0.74         94.33          3.43      ...         \n",
       "3           2.40          1.70         97.35          0.50      ...         \n",
       "4           2.76          0.53         89.16          1.17      ...         \n",
       "\n",
       "   burglaries  burglPerPop  larcenies  larcPerPop  autoTheft  autoTheftPerPop  \\\n",
       "0          14       114.85        138     1132.08         16           131.26   \n",
       "1          57       242.37        376     1598.78         26           110.55   \n",
       "2         274       758.14       1797     4972.19        136            376.3   \n",
       "3         225      1301.78        716     4142.56         47           271.93   \n",
       "4          91       728.93       1060     8490.87         91           728.93   \n",
       "\n",
       "   arsons  arsonsPerPop  ViolentCrimesPerPop  nonViolPerPop  \n",
       "0       2         16.41                41.02        1394.59  \n",
       "1       1          4.25               127.56        1955.95  \n",
       "2      22         60.87               218.59        6167.51  \n",
       "3       ?             ?               306.64              ?  \n",
       "4       5         40.05                    ?        9988.79  \n",
       "\n",
       "[5 rows x 147 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = contents_list # add headers with correct variable names\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onlyVarNames = [ v[2] for v in varNames ] # get the variables that we don't use bc they have too many NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2215, 147), (2215, 103))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.drop(onlyVarNames+['communityname', 'state'], axis=1) # drop vars that have a lot of NAs\n",
    "df2 = df2.drop(['fold'], axis=1)\n",
    "df.shape, df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>murders</th>\n",
       "      <th>murdPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11980</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>91.78</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1.88</td>\n",
       "      <td>12.47</td>\n",
       "      <td>21.44</td>\n",
       "      <td>10.93</td>\n",
       "      <td>11.33</td>\n",
       "      <td>...</td>\n",
       "      <td>53.72</td>\n",
       "      <td>65.29</td>\n",
       "      <td>78.09</td>\n",
       "      <td>89.14</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1845.9</td>\n",
       "      <td>9.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23123</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>95.57</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.85</td>\n",
       "      <td>11.01</td>\n",
       "      <td>21.30</td>\n",
       "      <td>10.48</td>\n",
       "      <td>17.18</td>\n",
       "      <td>...</td>\n",
       "      <td>77.17</td>\n",
       "      <td>71.27</td>\n",
       "      <td>90.22</td>\n",
       "      <td>96.12</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2186.7</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29344</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>94.33</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.35</td>\n",
       "      <td>11.36</td>\n",
       "      <td>25.88</td>\n",
       "      <td>11.01</td>\n",
       "      <td>10.28</td>\n",
       "      <td>...</td>\n",
       "      <td>44.77</td>\n",
       "      <td>36.60</td>\n",
       "      <td>61.26</td>\n",
       "      <td>82.85</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2780.9</td>\n",
       "      <td>4.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>8.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16656</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.70</td>\n",
       "      <td>97.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.70</td>\n",
       "      <td>12.55</td>\n",
       "      <td>25.20</td>\n",
       "      <td>12.19</td>\n",
       "      <td>17.57</td>\n",
       "      <td>...</td>\n",
       "      <td>88.71</td>\n",
       "      <td>56.70</td>\n",
       "      <td>90.17</td>\n",
       "      <td>96.24</td>\n",
       "      <td>5.2</td>\n",
       "      <td>3217.7</td>\n",
       "      <td>3.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11245</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.53</td>\n",
       "      <td>89.16</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.52</td>\n",
       "      <td>24.46</td>\n",
       "      <td>40.53</td>\n",
       "      <td>28.69</td>\n",
       "      <td>12.65</td>\n",
       "      <td>...</td>\n",
       "      <td>73.75</td>\n",
       "      <td>42.22</td>\n",
       "      <td>60.34</td>\n",
       "      <td>89.02</td>\n",
       "      <td>11.5</td>\n",
       "      <td>974.2</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>140494</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.51</td>\n",
       "      <td>95.65</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.95</td>\n",
       "      <td>18.09</td>\n",
       "      <td>32.89</td>\n",
       "      <td>20.04</td>\n",
       "      <td>13.26</td>\n",
       "      <td>...</td>\n",
       "      <td>64.35</td>\n",
       "      <td>42.29</td>\n",
       "      <td>70.61</td>\n",
       "      <td>85.66</td>\n",
       "      <td>70.4</td>\n",
       "      <td>1995.7</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28700</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.60</td>\n",
       "      <td>96.57</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.10</td>\n",
       "      <td>11.17</td>\n",
       "      <td>27.41</td>\n",
       "      <td>12.76</td>\n",
       "      <td>14.42</td>\n",
       "      <td>...</td>\n",
       "      <td>77.30</td>\n",
       "      <td>63.45</td>\n",
       "      <td>82.23</td>\n",
       "      <td>93.53</td>\n",
       "      <td>10.9</td>\n",
       "      <td>2643.5</td>\n",
       "      <td>9.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59459</td>\n",
       "      <td>2.45</td>\n",
       "      <td>14.20</td>\n",
       "      <td>84.87</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.63</td>\n",
       "      <td>15.31</td>\n",
       "      <td>27.93</td>\n",
       "      <td>14.78</td>\n",
       "      <td>14.60</td>\n",
       "      <td>...</td>\n",
       "      <td>73.70</td>\n",
       "      <td>54.85</td>\n",
       "      <td>85.55</td>\n",
       "      <td>91.51</td>\n",
       "      <td>39.2</td>\n",
       "      <td>1515.3</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "      <td>13.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>74111</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.35</td>\n",
       "      <td>97.11</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.73</td>\n",
       "      <td>16.64</td>\n",
       "      <td>35.16</td>\n",
       "      <td>20.33</td>\n",
       "      <td>8.58</td>\n",
       "      <td>...</td>\n",
       "      <td>58.82</td>\n",
       "      <td>40.72</td>\n",
       "      <td>67.97</td>\n",
       "      <td>81.39</td>\n",
       "      <td>30.9</td>\n",
       "      <td>2399.3</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>103590</td>\n",
       "      <td>2.62</td>\n",
       "      <td>23.14</td>\n",
       "      <td>67.60</td>\n",
       "      <td>0.92</td>\n",
       "      <td>16.35</td>\n",
       "      <td>19.88</td>\n",
       "      <td>34.55</td>\n",
       "      <td>21.62</td>\n",
       "      <td>13.12</td>\n",
       "      <td>...</td>\n",
       "      <td>75.59</td>\n",
       "      <td>42.33</td>\n",
       "      <td>74.05</td>\n",
       "      <td>92.12</td>\n",
       "      <td>78.5</td>\n",
       "      <td>1319.3</td>\n",
       "      <td>0.76</td>\n",
       "      <td>6.57</td>\n",
       "      <td>29</td>\n",
       "      <td>26.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0       11980           3.10          1.37         91.78          6.50   \n",
       "1       23123           2.82          0.80         95.57          3.44   \n",
       "2       29344           2.43          0.74         94.33          3.43   \n",
       "3       16656           2.40          1.70         97.35          0.50   \n",
       "4       11245           2.76          0.53         89.16          1.17   \n",
       "5      140494           2.45          2.51         95.65          0.90   \n",
       "6       28700           2.60          1.60         96.57          1.47   \n",
       "7       59459           2.45         14.20         84.87          0.40   \n",
       "8       74111           2.46          0.35         97.11          1.25   \n",
       "9      103590           2.62         23.14         67.60          0.92   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up     ...      \\\n",
       "0         1.88        12.47        21.44        10.93       11.33     ...       \n",
       "1         0.85        11.01        21.30        10.48       17.18     ...       \n",
       "2         2.35        11.36        25.88        11.01       10.28     ...       \n",
       "3         0.70        12.55        25.20        12.19       17.57     ...       \n",
       "4         0.52        24.46        40.53        28.69       12.65     ...       \n",
       "5         0.95        18.09        32.89        20.04       13.26     ...       \n",
       "6         1.10        11.17        27.41        12.76       14.42     ...       \n",
       "7         0.63        15.31        27.93        14.78       14.60     ...       \n",
       "8         0.73        16.64        35.16        20.33        8.58     ...       \n",
       "9        16.35        19.88        34.55        21.62       13.12     ...       \n",
       "\n",
       "   PctBornSameState  PctSameHouse85  PctSameCity85  PctSameState85  LandArea  \\\n",
       "0             53.72           65.29          78.09           89.14       6.5   \n",
       "1             77.17           71.27          90.22           96.12      10.6   \n",
       "2             44.77           36.60          61.26           82.85      10.6   \n",
       "3             88.71           56.70          90.17           96.24       5.2   \n",
       "4             73.75           42.22          60.34           89.02      11.5   \n",
       "5             64.35           42.29          70.61           85.66      70.4   \n",
       "6             77.30           63.45          82.23           93.53      10.9   \n",
       "7             73.70           54.85          85.55           91.51      39.2   \n",
       "8             58.82           40.72          67.97           81.39      30.9   \n",
       "9             75.59           42.33          74.05           92.12      78.5   \n",
       "\n",
       "   PopDens  PctUsePubTrans  LemasPctOfficDrugUn  murders  murdPerPop  \n",
       "0   1845.9            9.63                 0.00        0        0.00  \n",
       "1   2186.7            3.84                 0.00        0        0.00  \n",
       "2   2780.9            4.37                 0.00        3        8.30  \n",
       "3   3217.7            3.31                 0.00        0        0.00  \n",
       "4    974.2            0.38                 0.00        0        0.00  \n",
       "5   1995.7            0.97                 0.00        7        4.63  \n",
       "6   2643.5            9.62                 0.00        0        0.00  \n",
       "7   1515.3            0.70                 0.00        8       13.13  \n",
       "8   2399.3            1.41                 0.00        0        0.00  \n",
       "9   1319.3            0.76                 6.57       29       26.88  \n",
       "\n",
       "[10 rows x 103 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2215, 103)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check that there are no '?'s (NAs)\n",
    "df2 = df2.replace('?', np.nan)\n",
    "df2 = df2.dropna(axis=0)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1994, 103), (1994,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df2[df.ViolentCrimesPerPop != '?'] # didn't get rid of '?' in the y (ViolentCrimesPerPop) yet\n",
    "y = df.ViolentCrimesPerPop[df.ViolentCrimesPerPop != '?']\n",
    "y = pd.Series([float(a) > AVG_CRIME for a in y ]) # make the y 0 or 1\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X.dtypes # check that datatypes are numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1595, 103), (399, 103), (1595,), (399,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=364)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=20, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_rf = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random forest \n",
    "cm1 = confusion_matrix(y_test, predicted_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.87468672,  0.87468672]),\n",
       " 'FDR': array([ 0.10526316,  0.1754386 ]),\n",
       " 'FNR': array([ 0.07272727,  0.24193548]),\n",
       " 'FPR': array([ 0.24193548,  0.07272727]),\n",
       " 'NPV': array([ 0.8245614 ,  0.89473684]),\n",
       " 'PPV': array([ 0.89473684,  0.8245614 ]),\n",
       " 'TNR': array([ 0.75806452,  0.92727273]),\n",
       " 'TPR': array([ 0.92727273,  0.75806452])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistical_measures(cm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tiffanyzhu/miniconda3/lib/python3.5/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 3 racePctWhite (0.042743)\n",
      "2. feature 43 PctKids2Par (0.042022)\n",
      "3. feature 2 racepctblack (0.037905)\n",
      "4. feature 42 PctFam2Par (0.035855)\n",
      "5. feature 49 PctKidsBornNeverMar (0.035196)\n",
      "6. feature 44 PctYoungKids2Par (0.030858)\n",
      "7. feature 102 murdPerPop (0.024742)\n",
      "8. feature 15 pctWInvInc (0.022869)\n",
      "9. feature 17 pctWPubAsst (0.021122)\n",
      "10. feature 76 PctHousNoPhone (0.020522)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqwAAAJkCAYAAADUeXPpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCawAAFTaH+DotMo9TIrSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1145535c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "#X_arr = feature_matrix_clean\n",
    "#y_arr = [ (1 if (x > AVG_CRIME) else 0) for x in target_vector1_clean]\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "#num_attributes = len(X_arr[0])\n",
    "top_x = 10 # just get top 10\n",
    "for f in range(top_x):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f], df2.columns[indices[f]], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(top_x), importances[indices[:top_x]],\n",
    "       color=\"r\", yerr=std[indices[:top_x]], align=\"center\")\n",
    "plt.xticks(range(top_x), [df2.columns[indices[i]] for i in range(top_x)], rotation = 90)\n",
    "plt.xlim([-1, top_x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also with random forests, we can look at variable importance. The plot above shows the top 10 most important variables. We get that racePctWhite is the most important, so the percentage of caucasians in the city seems to be the most important factor. pctKids2Par is the percentage of kids in family housing with two parents, next is \n",
    "racepctblack (percentage of population that is african american). Next is PctFam2Par, percentage of families (with kids) that are headed by two parents. The fifth most important variables is PctKidsBornNeverMar, percentage of kids born to never married. Looking at the graph and numbers though, it seems that the importance values are actually all quite close and the most important is not so far from the second most important variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856569709127\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.88      0.92      0.90      1362\n",
      "       True       0.81      0.72      0.76       632\n",
      "\n",
      "avg / total       0.85      0.86      0.85      1994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random forest w/ CV\n",
    "predicted_rf_cv = cross_validation.cross_val_predict(RandomForestClassifier(n_estimators=20), X, y, cv=10)\n",
    "print(metrics.accuracy_score(y, predicted_rf_cv))\n",
    "print(metrics.classification_report(y, predicted_rf_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.85656971,  0.85656971]),\n",
       " 'FDR': array([ 0.12430168,  0.19217082]),\n",
       " 'FNR': array([ 0.07929515,  0.28164557]),\n",
       " 'FPR': array([ 0.28164557,  0.07929515]),\n",
       " 'NPV': array([ 0.80782918,  0.87569832]),\n",
       " 'PPV': array([ 0.87569832,  0.80782918]),\n",
       " 'TNR': array([ 0.71835443,  0.92070485]),\n",
       " 'TPR': array([ 0.92070485,  0.71835443])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm1b = confusion_matrix(y, predicted_rf_cv)\n",
    "statistical_measures(cm1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Using the top 10 most important variables\n",
    "\n",
    "From the variable importance plots, we try just using the top 10 variables to see how good of predictions we can get. Below, we use random forest and logistic regression and get similar results, the accuracy is around 0.85. This is worse than using logistic regression on all of the variables, but virtually the same as using random forest with all of the variables. This result makes sense since we are taking the top variables from generated by fitting a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>PctKids2Par</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>PctFam2Par</th>\n",
       "      <th>PctKidsBornNeverMar</th>\n",
       "      <th>PctYoungKids2Par</th>\n",
       "      <th>murdPerPop</th>\n",
       "      <th>pctWInvInc</th>\n",
       "      <th>pctWInvInc</th>\n",
       "      <th>pctWPubAsst</th>\n",
       "      <th>PctHousNoPhone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.78</td>\n",
       "      <td>90.17</td>\n",
       "      <td>1.37</td>\n",
       "      <td>91.43</td>\n",
       "      <td>0.36</td>\n",
       "      <td>95.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.20</td>\n",
       "      <td>70.20</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.57</td>\n",
       "      <td>85.33</td>\n",
       "      <td>0.80</td>\n",
       "      <td>86.91</td>\n",
       "      <td>0.24</td>\n",
       "      <td>96.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.11</td>\n",
       "      <td>64.11</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   racePctWhite  PctKids2Par  racepctblack  PctFam2Par  PctKidsBornNeverMar  \\\n",
       "0         91.78        90.17          1.37       91.43                 0.36   \n",
       "1         95.57        85.33          0.80       86.91                 0.24   \n",
       "\n",
       "   PctYoungKids2Par  murdPerPop  pctWInvInc  pctWInvInc  pctWPubAsst  \\\n",
       "0             95.78         0.0       70.20       70.20         1.03   \n",
       "1             96.82         0.0       64.11       64.11         2.75   \n",
       "\n",
       "   PctHousNoPhone  \n",
       "0            0.00  \n",
       "1            0.31  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = df2[['racePctWhite','PctKids2Par','racepctblack','PctFam2Par', \n",
    "              'PctKidsBornNeverMar', 'PctYoungKids2Par', 'murdPerPop',\n",
    "             'pctWInvInc','pctWInvInc','pctWPubAsst', 'PctHousNoPhone']]\n",
    "df_sub.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.85155466,  0.85155466]),\n",
       " 'FDR': array([ 0.12411848,  0.20833333]),\n",
       " 'FNR': array([ 0.08810573,  0.27848101]),\n",
       " 'FPR': array([ 0.27848101,  0.08810573]),\n",
       " 'NPV': array([ 0.79166667,  0.87588152]),\n",
       " 'PPV': array([ 0.87588152,  0.79166667]),\n",
       " 'TNR': array([ 0.72151899,  0.91189427]),\n",
       " 'TPR': array([ 0.91189427,  0.72151899])}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sub = df_sub[df.ViolentCrimesPerPop != '?'] # didn't get rid of '?' in the y (ViolentCrimesPerPop) yet\n",
    "y_sub = df.ViolentCrimesPerPop[df.ViolentCrimesPerPop != '?']\n",
    "y_sub = y # make the y 0 or 1\n",
    "\n",
    "predicted_rf_cv_sub = cross_validation.cross_val_predict(RandomForestClassifier(n_estimators=20), X_sub, y_sub, cv=10)\n",
    "cm_sub = confusion_matrix(y_sub, predicted_rf_cv_sub)\n",
    "statistical_measures(cm_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.84603811,  0.84603811]),\n",
       " 'FDR': array([ 0.13137666,  0.21136767]),\n",
       " 'FNR': array([ 0.08737151,  0.29746835]),\n",
       " 'FPR': array([ 0.29746835,  0.08737151]),\n",
       " 'NPV': array([ 0.78863233,  0.86862334]),\n",
       " 'PPV': array([ 0.86862334,  0.78863233]),\n",
       " 'TNR': array([ 0.70253165,  0.91262849]),\n",
       " 'TPR': array([ 0.91262849,  0.70253165])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_rf_cv_sub = cross_validation.cross_val_predict(RandomForestClassifier(n_estimators=20), X_sub, y_sub, cv=10)\n",
    "cm_sub = confusion_matrix(y_sub, predicted_rf_cv_sub)\n",
    "statistical_measures(cm_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Trying Subsets of the Variables\n",
    "In all of section 3, we used all of the variables (except for the variables that contained mostly NAs). Now we are interested in looking at different subsets, such as variables relating to education, race, etc. to see if we can make accurate predictions with less variables.\n",
    "\n",
    "### 4.1 Education\n",
    "First we try education related variables, PctLess9thGrade (the percentage of people 25 and over with less than a 9th grade education), PctNotHsGrad (percentage of people 25 and over that are not high school graduates), and PctBSorMore (percentage of people 25 and over with a bachelors degree of higher education). Below, we will find that just using education gives a much lower accuracy than what we have seen from other models that use more attributes. We used random forest and logistic regression, both give similar results, logistic regression is slightly better. We'll analyze the slightly better model (logistic regression), the accuracy is around 0.75 (error is around 0.25, quite close to the base rate of 0.31). The false negative rate is also quite high at 0.54, that is also concerning to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PctLess9thGrade</th>\n",
       "      <th>PctNotHSGrad</th>\n",
       "      <th>PctBSorMore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.81</td>\n",
       "      <td>9.90</td>\n",
       "      <td>48.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.61</td>\n",
       "      <td>13.72</td>\n",
       "      <td>29.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.80</td>\n",
       "      <td>9.09</td>\n",
       "      <td>30.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.05</td>\n",
       "      <td>33.68</td>\n",
       "      <td>10.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.15</td>\n",
       "      <td>23.06</td>\n",
       "      <td>25.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PctLess9thGrade  PctNotHSGrad  PctBSorMore\n",
       "0             5.81          9.90        48.18\n",
       "1             5.61         13.72        29.89\n",
       "2             2.80          9.09        30.13\n",
       "3            11.05         33.68        10.81\n",
       "4            12.15         23.06        25.28"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ed = df2[['PctLess9thGrade','PctNotHSGrad','PctBSorMore']]\n",
    "df_ed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1994, 3), (1994,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ed = df_ed[df.ViolentCrimesPerPop != '?'] # didn't get rid of '?' in the y (ViolentCrimesPerPop) yet\n",
    "y_ed = df.ViolentCrimesPerPop[df.ViolentCrimesPerPop != '?']\n",
    "y_ed = y # make the y 0 or 1\n",
    "X_ed.shape, y_ed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.72768305,  0.72768305]),\n",
       " 'FDR': array([ 0.22199593,  0.41458733]),\n",
       " 'FNR': array([ 0.15859031,  0.51740506]),\n",
       " 'FPR': array([ 0.51740506,  0.15859031]),\n",
       " 'NPV': array([ 0.58541267,  0.77800407]),\n",
       " 'PPV': array([ 0.77800407,  0.58541267]),\n",
       " 'TNR': array([ 0.48259494,  0.84140969]),\n",
       " 'TPR': array([ 0.84140969,  0.48259494])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_rf_cv_ed = cross_validation.cross_val_predict(RandomForestClassifier(n_estimators=20), X_ed, y_ed, cv=10)\n",
    "cm_ed = confusion_matrix(y_ed, predicted_rf_cv_ed)\n",
    "statistical_measures(cm_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.74974925,  0.74974925]),\n",
       " 'FDR': array([ 0.22215068,  0.34920635]),\n",
       " 'FNR': array([ 0.11306902,  0.54588608]),\n",
       " 'FPR': array([ 0.54588608,  0.11306902]),\n",
       " 'NPV': array([ 0.65079365,  0.77784932]),\n",
       " 'PPV': array([ 0.77784932,  0.65079365]),\n",
       " 'TNR': array([ 0.45411392,  0.88693098]),\n",
       " 'TPR': array([ 0.88693098,  0.45411392])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ed = cross_validation.cross_val_predict(linear_model.LogisticRegression(penalty='l2'), X_ed, y_ed, cv=10)\n",
    "cm_ed2 = confusion_matrix(y_ed, predicted_ed)\n",
    "statistical_measures(cm_ed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Family Information\n",
    "\n",
    "We next explore information about families and how this might affect crime rates. We will look a variables such as the percentage of male and females who are diveroced or never married, the percent of familes with kids (under 4, teenagers) and kids who do not have parents who are married."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MalePctDivorce</th>\n",
       "      <th>MalePctNevMarr</th>\n",
       "      <th>FemalePctDiv</th>\n",
       "      <th>TotalPctDiv</th>\n",
       "      <th>PersPerFam</th>\n",
       "      <th>PctFam2Par</th>\n",
       "      <th>PctKids2Par</th>\n",
       "      <th>PctYoungKids2Par</th>\n",
       "      <th>PctTeen2Par</th>\n",
       "      <th>PctWorkMomYoungKids</th>\n",
       "      <th>PctWorkMom</th>\n",
       "      <th>NumKidsBornNeverMar</th>\n",
       "      <th>PctKidsBornNeverMar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.67</td>\n",
       "      <td>26.38</td>\n",
       "      <td>5.22</td>\n",
       "      <td>4.47</td>\n",
       "      <td>3.22</td>\n",
       "      <td>91.43</td>\n",
       "      <td>90.17</td>\n",
       "      <td>95.78</td>\n",
       "      <td>95.81</td>\n",
       "      <td>44.56</td>\n",
       "      <td>58.88</td>\n",
       "      <td>31</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.23</td>\n",
       "      <td>27.99</td>\n",
       "      <td>6.45</td>\n",
       "      <td>5.42</td>\n",
       "      <td>3.11</td>\n",
       "      <td>86.91</td>\n",
       "      <td>85.33</td>\n",
       "      <td>96.82</td>\n",
       "      <td>86.46</td>\n",
       "      <td>51.14</td>\n",
       "      <td>62.43</td>\n",
       "      <td>43</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.10</td>\n",
       "      <td>25.78</td>\n",
       "      <td>14.76</td>\n",
       "      <td>12.55</td>\n",
       "      <td>2.95</td>\n",
       "      <td>78.54</td>\n",
       "      <td>78.85</td>\n",
       "      <td>92.37</td>\n",
       "      <td>75.72</td>\n",
       "      <td>66.08</td>\n",
       "      <td>74.19</td>\n",
       "      <td>164</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.98</td>\n",
       "      <td>28.15</td>\n",
       "      <td>14.47</td>\n",
       "      <td>12.91</td>\n",
       "      <td>2.98</td>\n",
       "      <td>64.02</td>\n",
       "      <td>62.36</td>\n",
       "      <td>65.38</td>\n",
       "      <td>67.43</td>\n",
       "      <td>59.59</td>\n",
       "      <td>70.27</td>\n",
       "      <td>561</td>\n",
       "      <td>3.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.51</td>\n",
       "      <td>50.66</td>\n",
       "      <td>11.64</td>\n",
       "      <td>9.73</td>\n",
       "      <td>2.98</td>\n",
       "      <td>58.59</td>\n",
       "      <td>55.20</td>\n",
       "      <td>66.51</td>\n",
       "      <td>79.17</td>\n",
       "      <td>61.22</td>\n",
       "      <td>68.94</td>\n",
       "      <td>402</td>\n",
       "      <td>4.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MalePctDivorce  MalePctNevMarr  FemalePctDiv  TotalPctDiv  PersPerFam  \\\n",
       "0            3.67           26.38          5.22         4.47        3.22   \n",
       "1            4.23           27.99          6.45         5.42        3.11   \n",
       "2           10.10           25.78         14.76        12.55        2.95   \n",
       "3           10.98           28.15         14.47        12.91        2.98   \n",
       "4            7.51           50.66         11.64         9.73        2.98   \n",
       "\n",
       "   PctFam2Par  PctKids2Par  PctYoungKids2Par  PctTeen2Par  \\\n",
       "0       91.43        90.17             95.78        95.81   \n",
       "1       86.91        85.33             96.82        86.46   \n",
       "2       78.54        78.85             92.37        75.72   \n",
       "3       64.02        62.36             65.38        67.43   \n",
       "4       58.59        55.20             66.51        79.17   \n",
       "\n",
       "   PctWorkMomYoungKids  PctWorkMom  NumKidsBornNeverMar  PctKidsBornNeverMar  \n",
       "0                44.56       58.88                   31                 0.36  \n",
       "1                51.14       62.43                   43                 0.24  \n",
       "2                66.08       74.19                  164                 0.88  \n",
       "3                59.59       70.27                  561                 3.84  \n",
       "4                61.22       68.94                  402                 4.70  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fam = df2.ix[:,37:50]\n",
    "df_fam.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_fam = df_fam[df.ViolentCrimesPerPop != '?'] # didn't get rid of '?' in the y (ViolentCrimesPerPop) yet\n",
    "y_fam = df.ViolentCrimesPerPop[df.ViolentCrimesPerPop != '?']\n",
    "y_fam = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.84503511,  0.84503511]),\n",
       " 'FDR': array([ 0.13664596,  0.20366972]),\n",
       " 'FNR': array([ 0.0814978 ,  0.31329114]),\n",
       " 'FPR': array([ 0.31329114,  0.0814978 ]),\n",
       " 'NPV': array([ 0.79633028,  0.86335404]),\n",
       " 'PPV': array([ 0.86335404,  0.79633028]),\n",
       " 'TNR': array([ 0.68670886,  0.9185022 ]),\n",
       " 'TPR': array([ 0.9185022 ,  0.68670886])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_rf_cv_fam = cross_validation.cross_val_predict(RandomForestClassifier(n_estimators=20), X_fam, y_fam, cv=10)\n",
    "cm_fam = confusion_matrix(y_fam, predicted_rf_cv_fam)\n",
    "statistical_measures(cm_fam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.8550652,  0.8550652]),\n",
       " 'FDR': array([ 0.1276891 ,  0.18987342]),\n",
       " 'FNR': array([ 0.07709251,  0.29113924]),\n",
       " 'FPR': array([ 0.29113924,  0.07709251]),\n",
       " 'NPV': array([ 0.81012658,  0.8723109 ]),\n",
       " 'PPV': array([ 0.8723109 ,  0.81012658]),\n",
       " 'TNR': array([ 0.70886076,  0.92290749]),\n",
       " 'TPR': array([ 0.92290749,  0.70886076])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_fam = cross_validation.cross_val_predict(linear_model.LogisticRegression(penalty='l2'), X_fam, y_fam, cv=10)\n",
    "cm_fam2 = confusion_matrix(y_fam, predicted_fam)\n",
    "statistical_measures(cm_fam2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using attributes related to family, we get better predictions than when using education variables. But we still see lower accuracies compared to when we used more attributes. Again, using logistic regression and random forests give similar results. Here, we get an error rate of around 0.145, much better than in section 4.1. Furthmore, an error rate around 0.145 is actually quite close to when we used a random forest on all the attributes (except the ones with mostly NAs) back in section 3.3. This may mean that family information can give indications of whether there will be more crime or not in a city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Household\n",
    "Next, we will look at variables related to households. For example, we will use percentage of the population with large households (more than 9 people living in the same house), how many vacant houses there, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PctLargHouseFam</th>\n",
       "      <th>PctLargHouseOccup</th>\n",
       "      <th>PersPerOccupHous</th>\n",
       "      <th>PersPerOwnOccHous</th>\n",
       "      <th>PersPerRentOccHous</th>\n",
       "      <th>PctPersOwnOccup</th>\n",
       "      <th>PctPersDenseHous</th>\n",
       "      <th>PctHousLess3BR</th>\n",
       "      <th>MedNumBR</th>\n",
       "      <th>HousVacant</th>\n",
       "      <th>...</th>\n",
       "      <th>RentLowQ</th>\n",
       "      <th>RentMedian</th>\n",
       "      <th>RentHighQ</th>\n",
       "      <th>RentQrange</th>\n",
       "      <th>MedRent</th>\n",
       "      <th>MedRentPctHousInc</th>\n",
       "      <th>MedOwnCostPctInc</th>\n",
       "      <th>MedOwnCostPctIncNoMtg</th>\n",
       "      <th>NumInShelters</th>\n",
       "      <th>NumStreet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.81</td>\n",
       "      <td>4.17</td>\n",
       "      <td>2.99</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.84</td>\n",
       "      <td>91.46</td>\n",
       "      <td>0.39</td>\n",
       "      <td>11.06</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>685</td>\n",
       "      <td>1001</td>\n",
       "      <td>1001</td>\n",
       "      <td>316</td>\n",
       "      <td>1001</td>\n",
       "      <td>23.8</td>\n",
       "      <td>21.1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.25</td>\n",
       "      <td>3.34</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.83</td>\n",
       "      <td>1.96</td>\n",
       "      <td>89.03</td>\n",
       "      <td>1.01</td>\n",
       "      <td>23.60</td>\n",
       "      <td>3</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>467</td>\n",
       "      <td>560</td>\n",
       "      <td>672</td>\n",
       "      <td>205</td>\n",
       "      <td>627</td>\n",
       "      <td>27.6</td>\n",
       "      <td>20.7</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PctLargHouseFam  PctLargHouseOccup  PersPerOccupHous  PersPerOwnOccHous  \\\n",
       "0             4.81               4.17              2.99               3.00   \n",
       "1             4.25               3.34              2.70               2.83   \n",
       "\n",
       "   PersPerRentOccHous  PctPersOwnOccup  PctPersDenseHous  PctHousLess3BR  \\\n",
       "0                2.84            91.46              0.39           11.06   \n",
       "1                1.96            89.03              1.01           23.60   \n",
       "\n",
       "   MedNumBR  HousVacant    ...      RentLowQ  RentMedian  RentHighQ  \\\n",
       "0         3          64    ...           685        1001       1001   \n",
       "1         3         240    ...           467         560        672   \n",
       "\n",
       "   RentQrange  MedRent  MedRentPctHousInc  MedOwnCostPctInc  \\\n",
       "0         316     1001               23.8              21.1   \n",
       "1         205      627               27.6              20.7   \n",
       "\n",
       "   MedOwnCostPctIncNoMtg  NumInShelters  NumStreet  \n",
       "0                   14.0             11          0  \n",
       "1                   12.5              0          0  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_h = df2.ix[:,61:92]\n",
    "df_h.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.83500502,  0.83500502]),\n",
       " 'FDR': array([ 0.14354727,  0.22201835]),\n",
       " 'FNR': array([ 0.08883994,  0.32911392]),\n",
       " 'FPR': array([ 0.32911392,  0.08883994]),\n",
       " 'NPV': array([ 0.77798165,  0.85645273]),\n",
       " 'PPV': array([ 0.85645273,  0.77798165]),\n",
       " 'TNR': array([ 0.67088608,  0.91116006]),\n",
       " 'TPR': array([ 0.91116006,  0.67088608])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_h = df_h[df.ViolentCrimesPerPop != '?'] # didn't get rid of '?' in the y (ViolentCrimesPerPop) yet\n",
    "y_h = df.ViolentCrimesPerPop[df.ViolentCrimesPerPop != '?']\n",
    "y_h = y\n",
    "\n",
    "predicted_rf_cv_h = cross_validation.cross_val_predict(RandomForestClassifier(n_estimators=20), X_h, y_h, cv=10)\n",
    "cm_h = confusion_matrix(y_h, predicted_rf_cv_h)\n",
    "statistical_measures(cm_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.81895687,  0.81895687]),\n",
       " 'FDR': array([ 0.16476892,  0.22954092]),\n",
       " 'FNR': array([ 0.08443465,  0.38924051]),\n",
       " 'FPR': array([ 0.38924051,  0.08443465]),\n",
       " 'NPV': array([ 0.77045908,  0.83523108]),\n",
       " 'PPV': array([ 0.83523108,  0.77045908]),\n",
       " 'TNR': array([ 0.61075949,  0.91556535]),\n",
       " 'TPR': array([ 0.91556535,  0.61075949])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_h = cross_validation.cross_val_predict(linear_model.LogisticRegression(penalty='l2'), X_h, y_h, cv=10)\n",
    "cm_h2 = confusion_matrix(y_h, predicted_h)\n",
    "statistical_measures(cm_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Income\n",
    "Next we look at income information like the median income in the city, percent of the population with a wage, percent of households that are self employed, percent of people who are unemployed, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medIncome</th>\n",
       "      <th>pctWWage</th>\n",
       "      <th>pctWFarmSelf</th>\n",
       "      <th>pctWInvInc</th>\n",
       "      <th>pctWSocSec</th>\n",
       "      <th>pctWPubAsst</th>\n",
       "      <th>pctWRetire</th>\n",
       "      <th>medFamInc</th>\n",
       "      <th>perCapInc</th>\n",
       "      <th>NumUnderPov</th>\n",
       "      <th>PctPopUnderPov</th>\n",
       "      <th>PctUnemployed</th>\n",
       "      <th>PctEmploy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75122</td>\n",
       "      <td>89.24</td>\n",
       "      <td>1.55</td>\n",
       "      <td>70.20</td>\n",
       "      <td>23.62</td>\n",
       "      <td>1.03</td>\n",
       "      <td>18.39</td>\n",
       "      <td>79584</td>\n",
       "      <td>29711</td>\n",
       "      <td>227</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.70</td>\n",
       "      <td>64.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47917</td>\n",
       "      <td>78.99</td>\n",
       "      <td>1.11</td>\n",
       "      <td>64.11</td>\n",
       "      <td>35.50</td>\n",
       "      <td>2.75</td>\n",
       "      <td>22.85</td>\n",
       "      <td>55323</td>\n",
       "      <td>20148</td>\n",
       "      <td>885</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>61.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   medIncome  pctWWage  pctWFarmSelf  pctWInvInc  pctWSocSec  pctWPubAsst  \\\n",
       "0      75122     89.24          1.55       70.20       23.62         1.03   \n",
       "1      47917     78.99          1.11       64.11       35.50         2.75   \n",
       "\n",
       "   pctWRetire  medFamInc  perCapInc  NumUnderPov  PctPopUnderPov  \\\n",
       "0       18.39      79584      29711          227            1.96   \n",
       "1       22.85      55323      20148          885            3.98   \n",
       "\n",
       "   PctUnemployed  PctEmploy  \n",
       "0           2.70      64.55  \n",
       "1           2.43      61.96  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in = df2[[ 'medIncome', 'pctWWage','pctWFarmSelf','pctWInvInc','pctWSocSec','pctWPubAsst','pctWRetire',\n",
    "           'medFamInc','perCapInc','NumUnderPov','PctPopUnderPov','PctUnemployed','PctEmploy']]\n",
    "df_in.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.83049147,  0.83049147]),\n",
       " 'FDR': array([ 0.14786795,  0.22777778]),\n",
       " 'FNR': array([ 0.09030837,  0.34018987]),\n",
       " 'FPR': array([ 0.34018987,  0.09030837]),\n",
       " 'NPV': array([ 0.77222222,  0.85213205]),\n",
       " 'PPV': array([ 0.85213205,  0.77222222]),\n",
       " 'TNR': array([ 0.65981013,  0.90969163]),\n",
       " 'TPR': array([ 0.90969163,  0.65981013])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_in = df_in[df.ViolentCrimesPerPop != '?'] # didn't get rid of '?' in the y (ViolentCrimesPerPop) yet\n",
    "y_in = df.ViolentCrimesPerPop[df.ViolentCrimesPerPop != '?']\n",
    "y_in = y\n",
    "\n",
    "predicted_rf_cv_in = cross_validation.cross_val_predict(RandomForestClassifier(n_estimators=20), X_in, y_in, cv=10)\n",
    "cm_in = confusion_matrix(y_in, predicted_rf_cv_in)\n",
    "statistical_measures(cm_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.82998997,  0.82998997]),\n",
       " 'FDR': array([ 0.14893617,  0.22718808]),\n",
       " 'FNR': array([ 0.08957416,  0.34335443]),\n",
       " 'FPR': array([ 0.34335443,  0.08957416]),\n",
       " 'NPV': array([ 0.77281192,  0.85106383]),\n",
       " 'PPV': array([ 0.85106383,  0.77281192]),\n",
       " 'TNR': array([ 0.65664557,  0.91042584]),\n",
       " 'TPR': array([ 0.91042584,  0.65664557])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_in = cross_validation.cross_val_predict(linear_model.LogisticRegression(penalty='l2'), X_in, y_in, cv=10)\n",
    "cm_in2 = confusion_matrix(y_in, predicted_in)\n",
    "statistical_measures(cm_in2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Race/Ethnicity\n",
    "From the random forest in section 3.3, we saw that race seemed to contribute the most. Now we will look at the variables related to race and see if we can make accurate predictions from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctHisp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.78</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.57</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   racePctWhite  racePctAsian  racepctblack  racePctHisp\n",
       "0         91.78          6.50          1.37         1.88\n",
       "1         95.57          3.44          0.80         0.85"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ra = df2[['racePctWhite','racePctAsian','racepctblack','racePctHisp']]\n",
    "df_ra.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.8109328,  0.8109328]),\n",
       " 'FDR': array([ 0.15679443,  0.27191413]),\n",
       " 'FNR': array([ 0.11160059,  0.35601266]),\n",
       " 'FPR': array([ 0.35601266,  0.11160059]),\n",
       " 'NPV': array([ 0.72808587,  0.84320557]),\n",
       " 'PPV': array([ 0.84320557,  0.72808587]),\n",
       " 'TNR': array([ 0.64398734,  0.88839941]),\n",
       " 'TPR': array([ 0.88839941,  0.64398734])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ra = df_ra[df.ViolentCrimesPerPop != '?'] # didn't get rid of '?' in the y (ViolentCrimesPerPop) yet\n",
    "y_ra = df.ViolentCrimesPerPop[df.ViolentCrimesPerPop != '?']\n",
    "y_ra = y\n",
    "\n",
    "predicted_rf_cv_ra = cross_validation.cross_val_predict(RandomForestClassifier(n_estimators=20), X_ra, y_ra, cv=10)\n",
    "cm_ra = confusion_matrix(y_ra, predicted_rf_cv_ra)\n",
    "statistical_measures(cm_ra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACC': array([ 0.82347041,  0.82347041]),\n",
       " 'FDR': array([ 0.16467463,  0.21311475]),\n",
       " 'FNR': array([ 0.0763583 ,  0.39240506]),\n",
       " 'FPR': array([ 0.39240506,  0.0763583 ]),\n",
       " 'NPV': array([ 0.78688525,  0.83532537]),\n",
       " 'PPV': array([ 0.83532537,  0.78688525]),\n",
       " 'TNR': array([ 0.60759494,  0.9236417 ]),\n",
       " 'TPR': array([ 0.9236417 ,  0.60759494])}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ra = cross_validation.cross_val_predict(linear_model.LogisticRegression(penalty='l2'), X_ra, y_ra, cv=10)\n",
    "cm_ra2 = confusion_matrix(y_ra, predicted_ra)\n",
    "statistical_measures(cm_ra2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at all of the subsets, family information, household information, income, and race information all perform about the same. However, just using education gives the worst and just family information does the best (for the subsets we tested). Looking at these subsets gives us good insight and tells us that we could make decent predictions with just these subsets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusions\n",
    "It was interesting to find that if we used all of the attributes (except for the ones with mostly NAs), most of the important variables were related to race. Several of the important variables were also relevant to family life but it is unsurprising that important variables are mostly related as they influence each other. However, it was surprising that none of the influential variables were related to income, for example there did not seem to variables about housing, employment or police that was \"important.\" However, we note that our model used many attributes and this in practice is not very practical because a city would need to survey a lot of information in order for our model to be useful. In the case of using all of the attributes, logistic regression seemed to be the best because it had a very low error rate and very low false negative rates. \n",
    "\n",
    "Then when we looked at just using specific varibles like just race or household, we got worse results but still OK. In the real world, perhaps we would not have all of the information we had in this data and instead only had information about the familes. From section 4.2, we saw that we could still get an accuracy of around 0.85. However, just using education information did not seem to give that great of predictions, accuracy of around 0.72. Nevertheless, our analysis from section 4 tells us that we could use subsets and get OK results because over 100 attributes is a lot, so using just 5 to 10 would probably be more feasible. \n",
    "\n",
    "Our models could be used by mayors if they want to see what factors in their city is contributing most to crime rates. For example, if it appeared that families with mostly unemployed members was an important variable in determining crime rates, the mayor could initialize more plans to create jobs. However, variables like the one we found to be the most important, percentage of Caucasians in the populaton, would not be very helpful to a mayor as one cannot control such things. Additionally it's important to realize that factors like percentage of a certain type of race that appeared to be one of the important factors may not actually be the cause of the crime rates. Also families may be interested in this model because if they want to move to another city, it may be important to them to see if a city has higher than average crime rates.\n",
    "\n",
    "\n",
    "## 6 Next Steps\n",
    "We could find a more recent dataset because the one we are using is from 1995 and may not be as relevant. However, perhaps the data is still OK since crime is not extremely volatile, it does not change much throughout, as opposed to something relating to technology. Our data set also was not very complete because it was missing a lot of the police information and perhaps that information could have been useful. We also could look for connections and similarities within subsets of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
